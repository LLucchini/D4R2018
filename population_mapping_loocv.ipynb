# Adopting the same techniques used in "population_mapping_mcmc.ipynb" we estimate leave onee out cross validation
# There are control parameters to select which model to use: "Normal" or "studentT"

import numpy as np
import pandas as pd
import pymc3 as pm

import matplotlib.pyplot as plt
%matplotlib inline

# ----------------------------------------------------------------------------------------------------- #

### Load densities' dataset
densities = pd.read_pickle('densities_province.pkl')

# ----------------------------------------------------------------------------------------------------- #

# Introduce models for bayesian inference
def model_mcmc_normal(X,Y):
    ### Definition of the MonteCarloMarkovChain model that will be used to optimize the value of the parameters
    """
      Params:
          X = pd.Series containing the mobile phone calls
          Y = pd.Series containing the population data
      Returns:
          population_model = pymc3 MCMC model 
    """    
    with pm.Model() as population_model:
        # Priors for the unknown model parameters
        alpha = pm.Normal('alpha', mu=10, sd=10)
        beta = pm.Normal('beta', mu=1, sd=1)
        sigma = pm.HalfNormal('sigma', sd=10)

        # Expected value of outcome
        mu = alpha+beta*np.log(X)

        # Likelihood (sampling distribution) of observations
        Y_obs = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=np.log(Y))
#         Y_obs = pm.StudentT('Y_obs', nu=3, mu=mu, observed=np.log(Y))
    
    return population_model

def model_mcmc_student_T(X,Y):
    ### Definition of the MonteCarloMarkovChain model that will be used to optimize the value of the parameters
    """
      Params:
          X = pd.Series containing the mobile phone calls
          Y = pd.Series containing the population data
      Returns:
          population_model = pymc3 MCMC model 
    """
    with pm.Model() as population_model:
        # Priors for the unknown model parameters
        alpha = pm.Normal('alpha', mu=10, sd=10)
        beta = pm.Normal('beta', mu=1, sd=1)
#         sigma = pm.HalfNormal('sigma', sd=10)

        # Expected value of outcome
        mu = alpha+beta*np.log(X)

        # Likelihood (sampling distribution) of observations
#         Y_obs = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=np.log(Y))
        Y_obs = pm.StudentT('Y_obs', nu=3, mu=mu, observed=np.log(Y))
    
    return population_model
    

# ----------------------------------------------------------------------------------------------------- #
# ------------------------------ First trial (with outgoing call data) -------------------------------- #
# ----------------------------------------------------------------------------------------------------- #
# We use MCCM to select the best model
# Leave One Out cross validation on provinces
Normal = False
studentT = True

data_size = len(densities)
pop_col = 'pop_density_t'
phone_col = 'calls_outgoing_density_t'

params_ = {}
params_['alpha'] = []
params_['beta'] = []
if Normal:
    params_['sigma'] = []
params_['test_out'] = []
params_['model_out'] = []
params_['r2'] = []


df_rand = densities.sample(data_size, axis=0)
for i in range(data_size):
    test = df_rand[df_rand.index==df_rand.index[i]]
    train = df_rand[df_rand.index!=df_rand.index[i]]
    
    X = train_in = np.array(train[phone_col])
    Y = train_out = np.array(train[pop_col])
    test_in = np.array(test[phone_col])
    test_out = np.log(np.array(test[pop_col]))
    
    # Initialize the baesyan model with the data
    if Normal:
        int_model = model_mcmc_normal(X,Y)
    elif studentT:
        int_model = model_mcmc_student_T(X,Y)
        
#     int_model = model_mcmc_student_T(X,Y)
    
    # Optimize the parameteres and store them in a dictionary
    # alpha, beta, and sigma
    # instantiate sampler: NUTS for continuos variables (since we are dealing with densities)
    step = pm.NUTS(model=int_model)

    # draw 5000 posterior samples -- burn-in with "tune" samples -- "njob" chains are run 
    # values for the params are stored in trace, the number of stored values are "njob"*5000
    trace = pm.sample(4000, model=int_model, step=step, tune=1000, njobs=4, chains=2, progressbar=False, )
    alpha = trace['alpha'].mean()
    beta = trace['beta'].mean()
    if Normal:
        sigma = trace['sigma'].mean()
    Y_loo = alpha+beta*np.log(test_in)
    Y_model = alpha+beta*np.log(X)
    r2 = pm.stats.r2_score(Y,Y_model)
    
    params_['alpha'].append(alpha)
    params_['beta'].append(beta)
    if Normal:
        params_['sigma'].append(sigma)
    params_['test_out'].append(test_out)
    params_['model_out'].append(Y_loo)
    params_['r2'].append(r2)
    print('Run #: {} - {}\r'.format(i+1,'|'*round(10*i/data_size)), end='')
#     pm.traceplot(trace)

# ----------------------------------------------------------------------------------------------------- #

# Mediated parameters using the LOOCV for the normal model
if Normal:
    params_normal = {}
    params_normal['LOOCV'] = np.sum((np.array(params_['model_out'])-np.array(params_['test_out']))**2)/len(df_rand)
    params_normal['alpha'] = (np.mean(params_['alpha']), np.std(params_['alpha']))
    params_normal['beta'] = (np.mean(params_['beta']), np.std(params_['beta']))
    params_normal['sigma'] = (np.mean(params_['sigma']), np.std(params_['sigma']))

# Mediated parameters using the LOOCV for the studentT model
if studentT:
    params_studT = {}
    params_studT['LOOCV'] = np.sum((np.array(params_['model_out'])-np.array(params_['test_out']))**2)/len(df_rand)
    params_studT['alpha'] = (np.mean(params_['alpha']), np.std(params_['alpha']))
    params_studT['beta'] = (np.mean(params_['beta']), np.std(params_['beta']))

# ----------------------------------------------------------------------------------------------------- #

